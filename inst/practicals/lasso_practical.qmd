---
title: "Practical 1: LASSO Logistic Regression in Epidemiology"
subtitle: "Overfitting, tuning, and (nested) cross-validation"
author: "Sten Willemsen"
format:
  html:
    toc: true
toc-depth: 3
execute:
  echo: true
warning: false
message: false
---
  
## Learning objectives
  
After this practical, you should be able to:
  
- Explain why LASSO can reduce overfitting
- Understand the role of the tuning parameter (λ)
- Apply cross-validation for model tuning
- Explain why **nested cross-validation** is required for honest performance estimates
- Reflect on the instability of variable selection in epidemiological data

---
  
## 1. Setup
  
We use **tidymodels** to ensure a consistent modeling workflow.

```{r}
library(tidymodels)
library(dplyr)

set.seed(2025)

PKGNAME <- "BigDataAndAI" # <--- CHANGE THIS to your package name

# Helper function to find if we are inside the package source tree
find_pkg_root <- function(path = getwd()) {
  # Traverse up directories looking for a DESCRIPTION file
  while (path != dirname(path)) {
    if (file.exists(file.path(path, "DESCRIPTION"))) {
      # Verify it is actually THIS package
      desc <- tryCatch(read.dcf(file.path(path, "DESCRIPTION")), error = function(e) NULL)
      if (!is.null(desc) && desc[1, "Package"] == PKGNAME) {
        return(path)
      }
    }
    path <- dirname(path)
  }
  return(NULL)
}

pkg_root <- find_pkg_root()

if (!is.null(pkg_root) && requireNamespace("devtools", quietly = TRUE)) {
  # SCENARIO 1: We are developing (inside the source folder)
  message(sprintf("Development mode: Loading %s from source via load_all()", PKGNAME))
  devtools::load_all(pkg_root, export_all = FALSE) # export_all=FALSE behaves more like real library
  
} else if (requireNamespace(PKGNAME, quietly = TRUE)) {
  # SCENARIO 2: Student mode (package is installed)
  message(sprintf("Student mode: Loading installed %s library", PKGNAME))
  library(PKGNAME, character.only = TRUE)
  
} else {
  # SCENARIO 3: Disaster (Not installed, not in source)
  stop(sprintf(
    "The package '%s' is not installed. Please run: remotes::install_github('stenw/%s')",
    PKGNAME, PKGNAME
  ))
}

```

Load the prepared NHANES dataset:
  
```{r}
nhanes <- load_nhanes()
```

---
  
## 2. Define outcome and predictors
  
  We deliberately include a **large set of predictors** to make overfitting a real concern.

> ⚠️ We are *not* claiming these variables are causal predictors.

```{r}
analysis_df <- nhanes %>%
  select(
    diabetes,
    age, sex, race_eth, education, income_pir,
    bmi,
    waist_cm,
    sleep_hours,
    sbp = sbp_mean,
    dbp = dbp_mean,
    glucose_mmol,
    LBXTC, LBDHDD, LBXTR,
    LBXSCR, alt, ast,
    smoking_status,
    vitD, uric_acid,
    alcohol,
    pa_total_weekly, sedentary_min,
    homa_ir,
    SIAPROXY, SIAINTRP, DMDMARTL,
    PHAFSTHR
  ) %>%
  filter(!is.na(diabetes))  %>%
  mutate(
    # Convert to factor
    diabetes = factor(diabetes, levels = c("1", "0")) # Force "1" to be first
  )

missing_frac <- analysis_df %>%
     summarise(across(
         everything(),
         ~ mean(is.na(.))
     )) %>%
     pivot_longer(
         everything(),
         names_to = "variable",
         values_to = "fraction_missing"
     ) %>%
     arrange(desc(fraction_missing))

glimpse(analysis_df)
```

**Questions:**  
- Why might a standard logistic regression struggle with this many correlated predictors, especially in smaller data sets ?
- NHANES is based on a complex survey design (each individual has a survey weight!). What are the implications of ignoring this for prediction modelling?
  
## 3. Train/test split 

To avoid information leakage and bias, data that is used for modelling cannot be used for evaluation of those models. We must therefore split the data. The package `rsample` helps us with this. Here we spit the data in train and test sets using the `initial_split` function. We can extract the training and testing data sets with the functions: `training` and `testing`. The test set is held out for final evaluation only. 
Here we use simple random sampling to split the data, but stratified sampling (e.g. `strata = "diabetes"`) is often preferred to ensure balanced class proportions in classification problems.

Other strategies for data splitting are available within the `rsample` package.
  
```{r splitdata}
set.seed(2025)
split <- rsample::initial_split(analysis_df, prop = 0.75)
train_data <- rsample::training(split)
test_data  <- rsample::testing(split)
```

Examine the resulting objects and the documentation of the `initial_split()` function to understand how the data is divided.

**Discussion:**  
  What goes wrong if we use the test set to choose lambda?
  
  ---
  

## 4. LASSO model specification

There are countless packages on CRAN that provide modelling functions implementing various types of models. Unfortunately, the interfaces of these functions are all different in how the data is passed to them (some expect `data.frames`, some `matrices`) and in terms of their arguments. The same is true for the predict methods of those functions (if they are implemented in the first place). The goal of the `parsnip` package within the tidymodels framework is to make this more unified so that users do not have to worry about the implementation. However as we will see later when we deal with neural networks, sometimes we still need to do some work to make certain packages behave the way the tidyverse expects them to. 

We now define a **logistic regression model**. As you see a distinction is made between the type of model (here `logistic_reg`) and the engine: (`glm`) is the software backend that actually fits the specified model using numerical algorithms.  

the underlying The model is specified but **not fitted yet**—it acts as a blueprint for later estimation. 

```{r}
lr_spec <- parsnip::logistic_reg() %>%
  parsnip::set_engine("glm")

lr_spec
```

When we have specified the model we can use the `fit()` function that will work the same for all kinds of models.


```{r}
lr_fit <- lr_spec |> 
  fit(diabetes ~ ., data=train_data)
```

Now we have fit the model to the training data. We can use the `predict()` function to generate predictions on new data or use the `tidy` function from the `broom` package to get a summary.

```{r}
predict(lr_fit, new_data=test_data[1:4,])
broom::tidy(lr_fit)
```

We now switch to lasso and introduce a penalty. We therefore switch the engine to `glmnet` which implements various types of penalized regression models including lasso and Ridge regression. 
Larger values of the parameter lambda lead to more shrinkage and fewer predictors in the model.  
`mixture = 1` specifies **pure LASSO** (L1 penalty), which performs automatic variable selection.  
The **engine** We want to optimize the value of lambda via cross-validation. Therefore we change the `penalty` argument to `tune()`.  This  means the strength of the LASSO penalty (lambda) will be **chosen by cross-validation**, not fixed in advance. 
  
```{r defspec}
lasso_spec <- parsnip::logistic_reg(
  penalty = tune(),   # lambda to be tuned
  mixture = 1         # pure LASSO
) %>%
  parsnip::set_engine("glmnet")

lasso_spec
```


## 5. Preprocessing recipe

Before fitting the model, we need to preprocess the data. This includes handling missing data, creating dummy variables for categorical predictors, and normalizing numeric predictors (as lasso is **scale-dependent**). We use the `recipes` package to define a preprocessing **recipe**. Preprocessing must generally done be *inside* resampling as it often learns parameters from the data (e.g., means and standard deviations for normalization). As such they can be seen as part of the whole modelling process. To be able to do a proper validation of the model no data dependent step must be excluded or we risk bias due to information leakage. 

The recipe defines the steps to be taken but does not apply them yet. When we want to apply the recipe we use the `prep()` function to learn any required parameters from the training data, and then the `bake()` function to apply the learned transformations to new data (see later on).

```{r}
lasso_recipe <- recipe(diabetes ~ ., data = analysis_df) %>%
  step_indicate_na(all_predictors()) %>%      
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_zv(all_predictors()) |> 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_normalize(all_numeric_predictors())  


lasso_recipe
```

In inferential statistics we often use multiple imputation to handle missingness. In prediction models it is often skipped. One reason is that missingness is often a predictor in itself (informative missingness). Furthermore using missingness indicators makes it relatively simple to apply the model to new data with may itself contain missing data (although there are also other methods that can solve this). Lastly multiple imputation can be computationally intensive, especially with large datasets and complex models.

**Question:**
- In `step_dummy` we use `one_hot = TRUE`. This makes indicator variables for all categories. Why is this no problem here?
- Why is normalization essential for LASSO but not strictly necessary for ordinary logistic regression? What would happen when we normalize on the whole data set?
- In a setting where we would have 1.000 observations would you use all these variables? What would happen to the standard errors.



  
## 6. Cross-validation for tuning lambda

Do find the best value of the hyperparameter lambda, we also need validation data. Therefore, we further divde the training data into folds for cross-validation. This is because we cannot use the same data for tuning as for testing. For this we use the `vfold_cv` function. 
  
### Inner cross-validation
  
```{r setup_cv_inner}
set.seed(2025)
cv_inner <- rsample::vfold_cv(train_data, v = 5)
```

### Workflow

Now we create a **workflow** object. a workflow is an object that combines data preprocessing (via a formula or recipe) with a model specification (and optionally a resampling or tuning context) into a single, reproducible unit. This ensures that all preprocessing steps are learned only from the training data and are applied consistently during fitting, resampling, and prediction. By bundling these steps together, workflows prevent common mistakes such as data leakage and make model development easier to reproduce and maintain.

```{r definewf}
lasso_wf <- workflow() %>%
  add_recipe(lasso_recipe) %>%
  add_model(lasso_spec)

lasso_wf
```

### Tuning grid

```{r definetuninggrid}
lambda_grid <- grid_regular(
  penalty(range = c(-5, 0)),  # 10^-5 to 1
  levels = 30
)
```


  
## 7. Tune the model

Let's look at the performance of the model for various values of lambda.
  
```{r tuning}
lasso_tuned <- tune_grid(
  lasso_wf,
  resamples = cv_inner,
  grid = lambda_grid,
  metrics = metric_set(roc_auc, mn_log_loss, brier_class)
)

lasso_tuned %>%
  collect_metrics() %>%
  filter(penalty < 0.025) %>%   # Filter out the "bad" models manually
  filter(.metric == "mn_log_loss") %>% # Focus on just one metric if you want
  ggplot(aes(x = penalty, y = mean)) +
  geom_line() +
  geom_point() +
  scale_x_log10() +
  labs(y = "Mean Log Loss", title = "Tuning Results (Zoomed In)")
```

```{r plottuning}
autoplot(lasso_tuned)
```

**Observation:**  
  Is there a single clearly optimal value of λ?
  
  ---
  
## 8. Finalize and evaluate 

We select the value of lambda that optimized the log-loss metric during tuning. "Finalizing" the workflow means updating it with this specific parameter value so it is no longer tunable. We then fit this finalized model to the entire training dataset and, finally, evaluate its performance on the held-out test set.
  
```{r select_best_lambda}
best_lambda <- select_best(lasso_tuned, metric="mn_log_loss")

final_lasso <- finalize_workflow(
  lasso_wf,
  best_lambda
)

final_fit <- fit(final_lasso, train_data)
```


```{r}
test_pred <- predict(final_fit, test_data, type = "prob") %>%
  bind_cols(test_data)

auc <- roc_auc(test_pred, truth = diabetes, .pred_1)
```

```{r}
test_pred %>%
  roc_curve(truth = diabetes, .pred_1) %>%
  autoplot() +
  labs(title = "ROC Curve (Test Set)",
        subtitle = paste0("AUC = ", round(auc$.estimate, 3))) 

```
```{r}
test_pred %>%
  mutate(prob_bin = ntile(.pred_1, 10)) %>% # Split into 10 bins
  group_by(prob_bin) %>%
  summarise(
    mean_pred = mean(.pred_1),
    # Convert factor "1"/"0" to numeric 1/0. 
    # If "1" is the first level, as.integer gives 1.
    mean_obs = mean(diabetes == "1"), 
    n = n()
  ) %>%
  ggplot(aes(x = mean_pred, y = mean_obs)) +
  geom_point() +
  geom_line() +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  labs(
    title = "Calibration Curve",
    x = "Predicted Probability",
    y = "Observed Proportion",
    caption = "Dashed line = Perfect Calibration"
  )
```

  
  
## 9. Nested cross-validation

While the train/test split gives us an unbiased estimate, it effectively reduces our sample size. As an alternative we can used an other layer of cross-validation.

- **Inner CV**: for tuning hyperparameters (in our case lambda)
- **Outer CV**: for performance estimation  


### Implementation
  
```{r}
set.seed(2025)
cv_outer <- vfold_cv(analysis_df, v = 5)
```

```{r nestedcv}
nested_results <- cv_outer %>%
  mutate(
    inner_cv = map(splits, ~ vfold_cv(analysis(.x), v = 5)),
    tuned = map(inner_cv, ~
                  tune_grid(
                    lasso_wf,
                    resamples = .x,
                    grid = lambda_grid,
                    metrics = metric_set(roc_auc, mn_log_loss, brier_class)
                  )
    ),
    best = map(tuned, select_best, metric="mn_log_loss"),
    final_wf = map(best, \(x) finalize_workflow(lasso_wf, x)),
    fitted = map2(final_wf, splits, \(x, y) fit(x, analysis(y))),
    assessed = map2(
      fitted, splits,
      \(x, y) predict(x, assessment(y), type = "prob") %>%
        bind_cols(assessment(y))
    ),
    auc = map_dbl(
      assessed,
      \(x) roc_auc(x, truth = diabetes, .pred_1)$.estimate
    )
  )
```

```{r resultscv}
nested_results$auc
mean(nested_results$auc)
```

## Overall ROC Curve

To construct an overall ROC curve, we pool the predictions from all outer test folds. Since each observation appears in exactly one outer test fold, these are all truly held-out predictions.

```{r pool-predictions}
# Combine all out-of-fold predictions
pooled_predictions <- nested_results %>%
  select(id, assessed) %>%
  unnest(assessed)


```

```{r roc-curve, fig.width=7, fig.height=6}
# Calculate ROC curve from pooled predictions
roc_curve_data <- pooled_predictions %>%
  roc_curve(truth = diabetes, .pred_1)

# Calculate overall AUC
overall_auc <- pooled_predictions %>%
  roc_auc(truth = diabetes, .pred_1)

# Plot ROC curve
roc_plot <- ggplot(roc_curve_data, aes(x = 1 - specificity, y = sensitivity)) +
  
  # Reference line (random classifier)
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray50") +
  
  # ROC curve
  geom_path(linewidth = 1, color = "#0072B2") +
  
  # Add AUC value as annotation
  annotate(
    "text",
    x = 0.75, y = 0.25,
    label = paste0("AUC = ", round(overall_auc$.estimate, 3)),
    size = 5, fontface = "bold"
  ) +
  
  # Labels and theme
  labs(
    title = "ROC Curve from Nested Cross-Validation",
    subtitle = "Pooled out-of-fold predictions from outer CV loop",
    x = "1 - Specificity (False Positive Rate)",
    y = "Sensitivity (True Positive Rate)"
  ) +
  coord_equal() +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )

roc_plot
```

Note that here we calculate the AUC a little bit different than before. Before we used macro averaging where we calculate the AUC for each fold and then average them. Here we use micro averaging where we pool all predictions and calculate a single AUC. There can be small differences between the two methods especially for measures like the AUC which are not calculated by averaging over individual predictions but rather by ranking them.


### Bootstrap of optimism

Note that there are also other ways to properly evaluate the model performance, for example we can create bootstap samples from the training data, fit the model on each bootstrap sample, and evaluate its performance on the training data as well as on the out-of-bag observations (those not included in the bootstrap sample). This gives us an estimate of the model optimism which we can use to correct the apparent performance on the training data. This method is often preferred, especially in smaller data sets but is a bit more complicated.

## 10. Variable selection instability

We use `extract_fit_parsnip` to extract the model fit from each outer fold, then we use the `tidy` function from the `broom` package to get the coefficients. We filter out the intercept and zero estimates, then pull the variable names. Finally, we tabulate how often each variable was selected across the outer folds.
  
```{r showmodelstab}
extract_vars <- function(fit) {
  broom::tidy(extract_fit_parsnip(fit)) %>%
    filter(term != "(Intercept)", estimate != 0) %>%
    pull(term)
}

selected_vars <- nested_results$fitted %>%
  map(extract_vars)

sort(table(unlist(selected_vars)), decreasing = TRUE)
```

**Reflection questions:**
  
- Why are some predictors selected inconsistently?
- What does this imply for etiological interpretation?
- How might this affect scientific conclusions?
  
---
  
## 11. Summary
  
- Tuning must be done via cross-validation
- Performance estimation can be done in an independent test set or ideally by **nested cross validation**.


In the next practical, we will compare LASSO with **tree-based boosting models** 


**Questions:**
  **Question:**
*  Many predictors in NHANES (e.g. smoking, alcohol use, physical activity) are self-reported.
  - Why might questionnaire-based variables still be useful for prediction, even if they are measured with error?
  - Why does the same measurement error pose a greater problem for causal interpretation?

*  Change the mixture parameter to 0. What kind of model does this correspond to?
