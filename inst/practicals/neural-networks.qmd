---
title: "Practical 3: Neural Networks in Epidemiology (torch)"
subtitle: "Training, tuning, cross-validation, and a lightweight nested CV"
author: "Sten Willemsen"
format:
  html:
    toc: true
    toc-depth: 3
execute:
  echo: true
  warning: false
  message: false
---

## Learning objectives

After this practical, you should be able to:

- Build a simple neural network (MLP) using **torch** in R
- Understand why neural networks can overfit (and how to mitigate it)
- Tune key hyperparameters (hidden units, dropout, learning rate)
- Use cross-validation and a **lightweight nested CV** for honest evaluation
- Reflect on when NNs are (and are not) a good choice in epidemiology

---

## 1. Setup

```{r}
library(dplyr)
library(readr)
library(tidymodels)   # for splits + recipes
library(torch)        # neural networks
library(yardstick)    # metrics

set.seed(2025)
torch_manual_seed(2025)

set.seed(2025)

PKGNAME <- "BigDataAndAI" # <--- CHANGE THIS to your package name

# Helper function to find if we are inside the package source tree
find_pkg_root <- function(path = getwd()) {
  # Traverse up directories looking for a DESCRIPTION file
  while (path != dirname(path)) {
    if (file.exists(file.path(path, "DESCRIPTION"))) {
      # Verify it is actually THIS package
      desc <- tryCatch(read.dcf(file.path(path, "DESCRIPTION")), error = function(e) NULL)
      if (!is.null(desc) && desc[1, "Package"] == PKGNAME) {
        return(path)
      }
    }
    path <- dirname(path)
  }
  return(NULL)
}

pkg_root <- find_pkg_root()

if (!is.null(pkg_root) && requireNamespace("devtools", quietly = TRUE)) {
  # SCENARIO 1: We are developing (inside the source folder)
  message(sprintf("Development mode: Loading %s from source via load_all()", PKGNAME))
  devtools::load_all(pkg_root, export_all = FALSE) # export_all=FALSE behaves more like real library
  
} else if (requireNamespace(PKGNAME, quietly = TRUE)) {
  # SCENARIO 2: Student mode (package is installed)
  message(sprintf("Student mode: Loading installed %s library", PKGNAME))
  library(PKGNAME, character.only = TRUE)
  
} else {
  # SCENARIO 3: Disaster (Not installed, not in source)
  stop(sprintf(
    "The package '%s' is not installed. Please run: remotes::install_github('stenw/%s')",
    PKGNAME, PKGNAME
  ))
}

```

Load the dataset:

```{r}
nhanes <- load_nhanes()
```

---

## 2. Define outcome and predictors

We use the same broad predictor set as the LASSO/boosting practicals.

```{r}
analysis_df <- nhanes %>%
  select(
    diabetes,
    age, sex, race_eth, education, income_pir,
    bmi,
    waist_cm,
    sleep_hours,
    sbp = sbp_mean,
    dbp = dbp_mean,
    glucose_mmol,
    LBXTC, LBDHDD, LBXTR,
    LBXSCR, alt, ast,
    smoking_status,
    vitD, uric_acid,
    alcohol,
    pa_total_weekly, sedentary_min,
    homa_ir,
    SIAPROXY, SIAINTRP, DMDMARTL,
    PHAFSTHR
  ) %>%
  filter(!is.na(diabetes))  %>%
  mutate(
    # Convert to factor
    diabetes = factor(diabetes, levels = c("1", "0")) # Force "1" to be first
  )

missing_frac <- analysis_df %>%
     summarise(across(
         everything(),
         ~ mean(is.na(.))
     )) %>%
     pivot_longer(
         everything(),
         names_to = "variable",
         values_to = "fraction_missing"
     ) %>%
     arrange(desc(fraction_missing))
```


## 3. Preprocessing


```{r}
nn_recipe <- recipe(diabetes ~ ., data = analysis_df) %>%
  step_indicate_na(all_predictors()) %>%      
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_zv(all_predictors()) |> 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_normalize(all_numeric_predictors())  


```

---

## 4. Utilities: recipe -> torch tensors

When we want to use torch to estimate neural networks in combination with the tidymodels methodology we need to take a few extra steps in comparison with the lasso and boosting models we used before. We are required to write a fuction (we call it `bake_xy` here) that takes a recipe and (training / test) data frames as input and returns the design matrices/vectors for training and testing.
We also create the function `to_tensor` which converts those matrices to 'torch tensors' which are are special 'arrays' that are used by torch as input format for neural networks. 
Finally we create a function `make_loader` which creates a 'dataloader' object from the tensors. A dataloader is an object that allows us to iterate over the data in mini-batches during training.

```{r}
bake_xy <- function(rec, train_df, test_df) {
  rec_prep <- prep(rec, training = train_df, retain = TRUE)

  x_train <- bake(rec_prep, new_data = train_df) %>%
    select(-diabetes) %>% as.matrix()
  y_train <- bake(rec_prep, new_data = train_df) %>%
    pull(diabetes) %>% as.integer() - 1L  # factor(0,1) -> 0/1

  x_test <- bake(rec_prep, new_data = test_df) %>%
    select(-diabetes) %>% as.matrix()
  y_test <- bake(rec_prep, new_data = test_df) %>%
    pull(diabetes) %>% as.integer() - 1L

  list(
    x_train = x_train, y_train = y_train,
    x_test  = x_test,  y_test  = y_test
  )
}

to_tensor <- function(x_mat, y_vec, device) {
  x <- torch_tensor(x_mat, dtype = torch_float(), device = device)
  y <- torch_tensor(matrix(y_vec, ncol = 1), dtype = torch_float(), device = device)
  list(x = x, y = y)
}

make_loader <- function(x, y, batch_size = 128, shuffle = TRUE) {
  ds <- tensor_dataset(x, y)
  dataloader(ds, batch_size = batch_size, shuffle = shuffle)
}
```


## 5. Define a simple neural network (MLP)

We use a small MLP for binary classification:

- input layer -> hidden layer -> hidden layer -> output
- dropout for regularization
- We use `BCEWithLogitsLoss` that combines the sigmoid function with the Binary Cross-Entropy Loss for stability (we output logits). Note that the Binary Cross-Entropy Loss and the logistic loss are equivalent to the negative log-likelihood.

```{r}
mlp_net <- nn_module(
  "mlp_net",
  initialize = function(p, hidden1 = 12, hidden2 = 6, dropout = 0.2) {
    self$fc1 <- nn_linear(p, hidden1)
    self$fc2 <- nn_linear(hidden1, hidden2)
    self$out <- nn_linear(hidden2, 1)

    self$drop <- nn_dropout(dropout)
  },
  forward = function(x) {
    x %>%
      self$fc1() %>%
      nnf_relu() %>%
      self$drop() %>%
      self$fc2() %>%
      nnf_relu() %>%
      self$drop() %>%
      self$out()
  }
)
```

---

## 6. Training + evaluation functions

Training a neural network in torch involves a lot of manual steps: you have to convert data to tensors, reset gradients, compute loss, update weights. We wrap these steps in the function `train_one`.

These functions:
- train for a fixed number of epochs
- evaluate on validation data
- return predicted probabilities + AUC + Brier score

```{r}
train_one <- function(x_train, y_train, x_val, y_val,
                      hidden1 = 12, hidden2 = 6, dropout = 0.2,
                      lr = 1e-3, epochs = 30, batch_size = 128,
                      weight_decay = 0, device = torch_device("cpu")) {

  p <- ncol(x_train)

  # Neural networks don't understand R data frames. This step converts your data into Tensors (multi-dimensional matrices or arrays that live on the CPU or GPU).
  tr <- to_tensor(x_train, y_train, device)
  va <- to_tensor(x_val,   y_val,   device)
  
  # the loader gives the data in mini-batches during training
  train_loader <- make_loader(tr$x, tr$y, batch_size = batch_size, shuffle = TRUE)

  # We initialize the "blank" neural network
  model <- mlp_net(p = p, hidden1 = hidden1, hidden2 = hidden2, dropout = dropout)
  # When we use a GPU we load the (still untrained) model onto the GPU as well
  model$to(device = device)
  
  # indicate that we use the ADAM (Adaptive moment estimator) optimizer to minimize the loss. 
  # This is a variant of gradient descent that is popular for training neural networks.
  # It incorporates momentum, the gradient used is a weighted moving average of past gradients and an adaptive learning rate.
  opt <- optim_adam(model$parameters, lr = lr, weight_decay = weight_decay)
  loss_fn <- nn_bce_with_logits_loss()
  
  # Tell torch we are in training mode (important for dropout layers)
  model$train()
  # start epoch loop (an epoch is a full pass through the training data)
  for (ep in 1:epochs) {
    # loop through the data in batches
    coro::loop(for (b in train_loader) {
      opt$zero_grad() # initialize gradients to zero st the start of the batch
      logits <- model(b[[1]]) # forward pass of batch through the model this gives log odds
      loss <- loss_fn(logits, b[[2]]) # calculate loss
      # Now perform backwards pass through the model calculate gradients indicating how 
      # to change the weights to reduce the loss
      loss$backward() 
      opt$step() # update the weights
    })
  }

  # evaluate
  model$eval() # switch to evaluation mode
  with_no_grad({ # telkls torch that no gradients need to be tracked
    logits_val <- model(va$x) # pass vallidation data through the model giving log odds
    # convert the log odds to probabilities and move from GPU to standard R arrays
    prob_val <- torch_sigmoid(logits_val)$to(device = torch_device("cpu")) %>% as_array()
  })

  # yardstick expects a data frame with truth + estimate
  df_val <- tibble(
    truth = factor(y_val, levels = c(1,0)),
    .pred_1 = as.numeric(prob_val[,1])
  )

  auc <- roc_auc(df_val, truth = truth, .pred_1) %>% pull(.estimate)
  brier <- mean((df_val$.pred_1 - as.numeric(as.character(df_val$truth)))^2)

  list(auc = auc, brier = brier, prob = df_val$.pred_1)
}
```

---

## 7. Quick sanity check: one train/test split

This is just for intuition. The main result will come from nested CV later.

```{r}
set.seed(2025)
split <- initial_split(analysis_df, prop = 0.75)
train_df <- training(split)
test_df  <- testing(split)

xy <- bake_xy(nn_recipe, train_df, test_df)
```

Choose device (GPU if available):

```{r}
device <- if (cuda_is_available()) torch_device("cuda") else torch_device("cpu")
device
```

Train one model (default settings):

```{r}
res_split <- train_one(
  x_train = xy$x_train, y_train = xy$y_train,
  x_val   = xy$x_test,  y_val   = xy$y_test,
  hidden1 = 12, hidden2 = 8,
  dropout = 0.2,
  lr = 1e-3,
  epochs = 30,
  batch_size = 128,
  device = device
)

res_split$auc
res_split$brier
```


## 8. Cross-validation for tuning (inner CV)

We will tune a small set of hyperparameters:

- hidden layer sizes
- dropout
- learning rate

To keep runtime reasonable, we do **3-fold inner CV** and a **small grid**.

> If your laptop is slow: reduce `grid` or reduce `epochs`.

```{r}
set.seed(2025)
cv_inner <- vfold_cv(analysis_df, v = 3)
```

Define a small hyperparameter grid:

```{r}
grid <- tidyr::crossing(
  hidden1 = c(12, 18),
  hidden2 = c(6, 12),
  dropout = c(0.1, 0.3),
  lr = c(1e-3, 3e-4)
) %>%
  slice_sample(n = 8)  # keep it small on purpose

grid
```

Function to score one configuration via CV:

```{r}
cv_score_config <- function(cfg, folds, epochs = 25, batch_size = 128, device = torch_device("cpu")) {
  fold_metrics <- purrr::map_dbl(folds$splits, function(spl) {
    train_df <- analysis(spl)
    val_df   <- assessment(spl)

    xy <- bake_xy(nn_recipe, train_df, val_df)

    out <- train_one(
      x_train = xy$x_train, y_train = xy$y_train,
      x_val   = xy$x_test,  y_val   = xy$y_test,
      hidden1 = cfg$hidden1,
      hidden2 = cfg$hidden2,
      dropout = cfg$dropout,
      lr      = cfg$lr,
      epochs  = epochs,
      batch_size = batch_size,
      device = device
    )
    out$auc
  })

  tibble(
    hidden1 = cfg$hidden1,
    hidden2 = cfg$hidden2,
    dropout = cfg$dropout,
    lr = cfg$lr,
    mean_auc = mean(fold_metrics),
    sd_auc = sd(fold_metrics)
  )
}
```

Run CV tuning:

```{r}
device <- if (cuda_is_available()) torch_device("cuda") else torch_device("cpu")

tuning_results <- purrr::pmap_dfr(grid, function(hidden1, hidden2, dropout, lr) {
  cfg <- list(hidden1 = hidden1, hidden2 = hidden2, dropout = dropout, lr = lr)
  cv_score_config(cfg, cv_inner, epochs = 20, batch_size = 128, device = device)
})

tuning_results %>% arrange(desc(mean_auc))
```

Select the best:

```{r}
best_cfg <- tuning_results %>% arrange(desc(mean_auc)) %>% slice(1)
best_cfg
```


## 9. Lightweight nested cross-validation (main result)

### Concept
- Outer CV estimates performance
- Inner CV tunes hyperparameters
- This avoids optimistic bias

To keep it feasible during class, we use **3 outer folds** and **3 inner folds**, and the small grid.

```{r}
set.seed(2025)
cv_outer <- vfold_cv(analysis_df, v = 3)
```

Nested CV:

```{r}
nested <- cv_outer %>%
  mutate(
    inner = purrr::map(splits, ~ vfold_cv(analysis(.x), v = 3)),
    best = purrr::map(inner, function(inner_folds) {
      tr <- purrr::pmap_dfr(grid, function(hidden1, hidden2, dropout, lr) {
        cfg <- list(hidden1 = hidden1, hidden2 = hidden2, dropout = dropout, lr = lr)
        cv_score_config(cfg, inner_folds, epochs = 15, batch_size = 128, device = device)
      })
      tr %>% arrange(desc(mean_auc)) %>% slice(1)
    }),
    # Fit best config on outer analysis, evaluate on outer assessment
    auc = purrr::map2_dbl(best, splits, function(cfg_tbl, spl) {
      cfg <- as.list(cfg_tbl %>% select(hidden1, hidden2, dropout, lr))

      train_df <- analysis(spl)
      test_df  <- assessment(spl)
      xy <- bake_xy(nn_recipe, train_df, test_df)

      out <- train_one(
        x_train = xy$x_train, y_train = xy$y_train,
        x_val   = xy$x_test,  y_val   = xy$y_test,
        hidden1 = cfg$hidden1,
        hidden2 = cfg$hidden2,
        dropout = cfg$dropout,
        lr      = cfg$lr,
        epochs  = 20,
        batch_size = 128,
        device = device
      )
      out$auc
    })
  )

nested %>% transmute(fold = id, auc, best)
mean(nested$auc)
sd(nested$auc)
```


## 10. Reflection: should we use a neural network here?

Discuss in groups:

1. Did the NN clearly outperform LASSO and boosting?
2. How sensitive was performance to hyperparameters?
3. What is the interpretability cost?
4. What kinds of epidemiological datasets would *justify* a neural network?


## 11. Optional extensions (if time)

### A) Stronger regularization
Try:
- higher dropout (0.5)
- `weight_decay` (L2 penalty) in the optimizer

### B) More epochs + early stopping (conceptual)
Train longer, but stop when validation AUC stops improving.

### C) Transportability
Train on earlier cycles and test on later cycles (requires cycle-based split).

---

## Summary

- Neural networks can fit complex patterns but are sensitive to tuning and can overfit easily.
- Honest evaluation requires careful resampling (nested CV).
- In epidemiology, the trade-off between performance and interpretability is often central.
